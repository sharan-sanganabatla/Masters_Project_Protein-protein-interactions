{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sharan.sanganabatla/Final/functions.py:173: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if input(\"Create tokenizers? Enter y if this is new training data. y/n: \") is 'y': create_tokenizers(df_test)\n",
      "/Users/sharan.sanganabatla/Final/functions.py:394: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if pad is 'center':\n",
      "/Users/sharan.sanganabatla/Final/functions.py:408: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if pad is 'center':\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle5 as pickle\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, GlobalMaxPooling1D, Flatten\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding,  Concatenate, Lambda\n",
    "from keras.models import Model\n",
    "from sklearn.metrics import roc_auc_score,roc_curve, auc\n",
    "from numpy import random\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data used will be:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Human</th>\n",
       "      <th>Pathogen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[M, T, V, V, G, N, P, R, S, W, S, C, Q, W, L, ...</td>\n",
       "      <td>[M, K, A, K, L, L, V, L, L, Y, A, F, V, A, T, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[M, F, R, L, L, S, W, S, L, G, R, G, F, L, R, ...</td>\n",
       "      <td>[M, E, R, I, K, E, L, R, N, L, M, S, Q, S, R, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[M, A, A, A, A, A, T, K, I, L, L, C, L, P, L, ...</td>\n",
       "      <td>[M, S, L, L, T, E, V, E, T, P, T, R, S, E, W, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[M, F, W, K, F, D, L, H, T, S, S, H, L, D, T, ...</td>\n",
       "      <td>[M, S, L, L, T, E, V, E, T, P, I, R, N, E, W, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[M, R, P, M, R, I, F, V, N, D, D, R, H, V, M, ...</td>\n",
       "      <td>[M, D, V, N, P, T, L, L, F, L, K, V, P, A, Q, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6995</th>\n",
       "      <td>[M, L, V, L, F, E, T, S, V, G, Y, A, I, F, K, ...</td>\n",
       "      <td>[M, N, P, N, Q, K, I, I, T, I, G, S, I, C, L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6996</th>\n",
       "      <td>[M, E, P, P, R, G, P, P, A, N, G, A, E, P, S, ...</td>\n",
       "      <td>[M, D, V, N, P, T, L, L, F, L, K, V, P, A, Q, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6997</th>\n",
       "      <td>[M, S, G, D, H, L, H, N, D, S, Q, I, E, A, D, ...</td>\n",
       "      <td>[M, N, P, N, Q, K, I, I, T, I, G, S, I, C, M, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6998</th>\n",
       "      <td>[M, T, D, S, K, Y, F, T, T, N, K, K, G, E, I, ...</td>\n",
       "      <td>[M, S, L, L, T, E, V, E, T, Y, V, L, S, I, V, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6999</th>\n",
       "      <td>[M, T, D, S, K, Y, F, T, T, N, K, K, G, E, I, ...</td>\n",
       "      <td>[M, N, P, N, Q, K, I, I, T, I, G, S, I, C, M, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Human  \\\n",
       "0     [M, T, V, V, G, N, P, R, S, W, S, C, Q, W, L, ...   \n",
       "1     [M, F, R, L, L, S, W, S, L, G, R, G, F, L, R, ...   \n",
       "2     [M, A, A, A, A, A, T, K, I, L, L, C, L, P, L, ...   \n",
       "3     [M, F, W, K, F, D, L, H, T, S, S, H, L, D, T, ...   \n",
       "4     [M, R, P, M, R, I, F, V, N, D, D, R, H, V, M, ...   \n",
       "...                                                 ...   \n",
       "6995  [M, L, V, L, F, E, T, S, V, G, Y, A, I, F, K, ...   \n",
       "6996  [M, E, P, P, R, G, P, P, A, N, G, A, E, P, S, ...   \n",
       "6997  [M, S, G, D, H, L, H, N, D, S, Q, I, E, A, D, ...   \n",
       "6998  [M, T, D, S, K, Y, F, T, T, N, K, K, G, E, I, ...   \n",
       "6999  [M, T, D, S, K, Y, F, T, T, N, K, K, G, E, I, ...   \n",
       "\n",
       "                                               Pathogen  \n",
       "0     [M, K, A, K, L, L, V, L, L, Y, A, F, V, A, T, ...  \n",
       "1     [M, E, R, I, K, E, L, R, N, L, M, S, Q, S, R, ...  \n",
       "2     [M, S, L, L, T, E, V, E, T, P, T, R, S, E, W, ...  \n",
       "3     [M, S, L, L, T, E, V, E, T, P, I, R, N, E, W, ...  \n",
       "4     [M, D, V, N, P, T, L, L, F, L, K, V, P, A, Q, ...  \n",
       "...                                                 ...  \n",
       "6995  [M, N, P, N, Q, K, I, I, T, I, G, S, I, C, L, ...  \n",
       "6996  [M, D, V, N, P, T, L, L, F, L, K, V, P, A, Q, ...  \n",
       "6997  [M, N, P, N, Q, K, I, I, T, I, G, S, I, C, M, ...  \n",
       "6998  [M, S, L, L, T, E, V, E, T, Y, V, L, S, I, V, ...  \n",
       "6999  [M, N, P, N, Q, K, I, I, T, I, G, S, I, C, M, ...  \n",
       "\n",
       "[7000 rows x 2 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_data(D,randomize=False):\n",
    "    try:\n",
    "        with open(\"df_train_\"+str(D)+\"D.pkl\", \"rb\") as fh:\n",
    "          df_train = pickle.load(fh)\n",
    "    except:\n",
    "        df_train = pd.read_pickle(\"hyppi-train-data.pkl\")\n",
    "    try:\n",
    "        with open(\"df_test_\"+str(D)+\"D.pkl\", \"rb\") as fh:\n",
    "          df_test = pickle.load(fh)\n",
    "    except:\n",
    "        df_test = pd.read_pickle(\"hyppi-test-data.pkl\")\n",
    "    if randomize:\n",
    "        return f.shuff_together(df_train,df_test)\n",
    "    else:\n",
    "        return df_train,df_test\n",
    "\n",
    "df_train,df_test = load_data(1)\n",
    "print('The data used will be:')\n",
    "df_train[['Human','Pathogen']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting to 5D. This will take a few minutes\n",
      "Create tokenizers? Enter y if this is new training data. y/n: y\n",
      "Saved tokenizers as doubleip_tkrs\n",
      "Saved tokenizer as join_tkr\n",
      "Preprocessing...\n",
      "sentences are\n",
      "Converting to 5D. This will take a few minutes\n",
      "Create tokenizers? Enter y if this is new training data. y/n: n\n",
      "Preprocessing...\n",
      "sentences are\n"
     ]
    }
   ],
   "source": [
    "trains = f.preprocess(df_train)\n",
    "tests = f.preprocess(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  1/219 [..............................] - ETA: 7:41:51 - loss: 0.6761"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM_5D = 25\n",
    "VALIDATION_SPLIT = 0.2\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1\n",
    "DROP = 0.5\n",
    "threshold = 0.918\n",
    "MAX_SEQUENCE_LENGTH_5D_J = 2000\n",
    "MAX_SEQUENCE_LENGTH_5D_dIP = 1000\n",
    "num_words_5D_J = 10000000\n",
    "num_words_5D = 5000000\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "\n",
    "  x1_join = f.transf_model(MAX_SEQUENCE_LENGTH_5D_J,num_words_5D_J,5,0.9)\n",
    "  x2_join = f.transf_model(MAX_SEQUENCE_LENGTH_5D_J,num_words_5D_J,5,0.9)\n",
    "  x3_join = f.transf_model(MAX_SEQUENCE_LENGTH_5D_J,num_words_5D_J,5,0.9)\n",
    "\n",
    "  x1_doubleip = f.conv_model(MAX_SEQUENCE_LENGTH_5D_dIP,EMBEDDING_DIM_5D,num_words_5D,DROP)\n",
    "  x2_doubleip = f.conv_model(MAX_SEQUENCE_LENGTH_5D_dIP,EMBEDDING_DIM_5D,num_words_5D,DROP)\n",
    "  x3_doubleip = f.conv_model(MAX_SEQUENCE_LENGTH_5D_dIP,EMBEDDING_DIM_5D,num_words_5D,DROP)\n",
    "  x4_doubleip = f.conv_model(MAX_SEQUENCE_LENGTH_5D_dIP,EMBEDDING_DIM_5D,num_words_5D,DROP)\n",
    "  x5_doubleip = f.conv_model(MAX_SEQUENCE_LENGTH_5D_dIP,EMBEDDING_DIM_5D,num_words_5D,DROP)\n",
    "  x6_doubleip = f.conv_model(MAX_SEQUENCE_LENGTH_5D_dIP,EMBEDDING_DIM_5D,num_words_5D,DROP)\n",
    "\n",
    "  concatenator = Concatenate(axis=1)\n",
    "  x = concatenator([x1_join.output, x2_join.output, x3_join.output, x1_doubleip.output, x2_doubleip.output, x3_doubleip.output, x4_doubleip.output, x5_doubleip.output, x6_doubleip.output])\n",
    "  x = Dense(256)(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  output = Dense(1, activation=\"sigmoid\",name=\"Final\")(x)\n",
    "  model_final = Model(inputs=[x1_join.input, x2_join.input, x3_join.input, x1_doubleip.input, x2_doubleip.input, x3_doubleip.input, x4_doubleip.input, x5_doubleip.input, x6_doubleip.input], outputs=output)\n",
    "\n",
    "  model_final.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "  model_final.fit(trains, df_train['label'].values, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(tests,df_test['label'].values))\n",
    "  score = roc_auc_score(df_test['label'].values, model_final.predict(tests))\n",
    "  score = round(score,3)\n",
    "  print(\"Score is\",score)\n",
    "  if score>threshold:\n",
    "    threshold = score\n",
    "    print('Saving model_final_'+str(score)+'')\n",
    "    model_final.save('model_final_'+str(i)+'_'+str(score)+'')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
